# Install necessary libraries
# !pip install -q transformers datasets soundfile accelerate huggingface_hub resampy torchaudio

import os
import io
import time
import json
import logging
import numpy as np
import torch
from functools import partial
from datasets import Dataset, Audio, load_dataset
from huggingface_hub import HfApi, HfFolder, get_full_repo_name, create_repo, CommitOperationAdd
import soundfile as sf
from torchaudio.transforms import Resample
import shutil
import traceback
import tempfile

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------

LOG_FILE_PATH = "/kaggle/working/processing_log.txt"

logging.basicConfig(
Â  Â  level=logging.INFO,
Â  Â  format='%(asctime)s - %(levelname)s - %(message)s',
Â  Â  handlers=[
Â  Â  Â  Â  logging.FileHandler(LOG_FILE_PATH, mode='w'),
Â  Â  Â  Â  logging.StreamHandler()
Â  Â  ]
)

# -----------------------------------------------------------------------------
# Configuration & Credentials
# -----------------------------------------------------------------------------

HF_TOKEN = os.environ.get("HF_TOKEN", HfFolder.get_token())
if not HF_TOKEN:
Â  Â  logging.critical("HF_TOKEN environment variable is not set. Please set your Hugging Face token as a Kaggle Secret.")
Â  Â  raise ValueError("HF_TOKEN environment variable is not set. Please set your Hugging Face token as a Kaggle Secret.")

TARGET_SR = 16000
STATE_FILE = "/kaggle/working/processed_samples.json"

# -----------------------------------------------------------------------------
# State Management
# -----------------------------------------------------------------------------

def load_processed_state():
Â  Â  """Loads the processing state from a JSON file."""
Â  Â  if os.path.exists(STATE_FILE):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  with open(STATE_FILE, 'r') as f:
Â  Â  Â  Â  Â  Â  Â  Â  return json.load(f)
Â  Â  Â  Â  except (json.JSONDecodeError, FileNotFoundError) as e:
Â  Â  Â  Â  Â  Â  logging.warning(f"Failed to load state file: {e}. Starting fresh.")
Â  Â  return {}

def save_processed_state(state):
Â  Â  """Saves the current processing state to a JSON file."""
Â  Â  with open(STATE_FILE, 'w') as f:
Â  Â  Â  Â  json.dump(state, f)

# -----------------------------------------------------------------------------
# Processing Functions
# -----------------------------------------------------------------------------

def is_normalized(sample, threshold=1e-6):
Â  Â  audio_data = sample['audio']['array']
Â  Â  if len(audio_data.shape) > 1:
Â  Â  Â  Â  audio_data = np.mean(audio_data, axis=1)
Â  Â  if abs(np.mean(audio_data)) > threshold or not np.isclose(np.max(np.abs(audio_data)), 1.0, atol=threshold):
Â  Â  Â  Â  return False
Â  Â  return True

def is_correct_sampling_rate(sample):
Â  Â  return sample['audio']['sampling_rate'] == TARGET_SR

def normalize_audio_sample(sample):
Â  Â  audio_data = sample['audio']['array']
Â  Â  if len(audio_data.shape) > 1:
Â  Â  Â  Â  audio_data = np.mean(audio_data, axis=1)
Â  Â  audio_data = audio_data - np.mean(audio_data)
Â  Â  peak_amp = np.max(np.abs(audio_data))
Â  Â  if peak_amp > 1e-6:
Â  Â  Â  Â  audio_data = audio_data / peak_amp
Â  Â  sample['audio']['array'] = audio_data
Â  Â  return sample

def resample_audio_sample(sample):
Â  Â  audio = sample['audio']
Â  Â  audio_data = audio['array']
Â  Â  current_sr = audio['sampling_rate']
Â  Â  if current_sr != TARGET_SR:
Â  Â  Â  Â  resampler = Resample(orig_freq=current_sr, new_freq=TARGET_SR)
Â  Â  Â  Â  audio_tensor = torch.from_numpy(audio_data).float()
Â  Â  Â  Â  audio_data = resampler(audio_tensor).numpy()
Â  Â Â 
Â  Â  sample['audio']['array'] = audio_data
Â  Â  sample['audio']['sampling_rate'] = TARGET_SR
Â  Â  return sample

# -----------------------------------------------------------------------------
# Main Orchestration Logic
# -----------------------------------------------------------------------------
def process_and_push_dataset(src_dataset_name, full_dest_repo_name, num_workers=4, batch_size=500):
Â  Â  api = HfApi()
Â  Â Â 
Â  Â  splits = ['train', 'validation', 'test']
Â  Â  state = load_processed_state()
Â  Â Â 
Â  Â  for split_name in splits:
Â  Â  Â  Â  processed_count = state.get(split_name, 0)
Â  Â  Â  Â  logging.info(f"\n--- Starting processing for split: '{split_name}' ---")
Â  Â  Â  Â  logging.info(f"Resuming from sample {processed_count}.")

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  ds = load_dataset(src_dataset_name, split=split_name, streaming=True)
Â  Â  Â  Â  Â  Â  ds_non_stream = load_dataset(src_dataset_name, split=split_name)
Â  Â  Â  Â  Â  Â  total_samples = len(ds_non_stream)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logging.error(f"Failed to load dataset split '{split_name}': {e}")
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  logging.info(f"Loaded '{split_name}' split with {total_samples} samples.")
Â  Â  Â  Â  logging.info(f"Starting processing and pushing to new repository for '{split_name}'...")
Â  Â  Â  Â Â 
Â  Â  Â  Â  ds_iterator = iter(ds)
Â  Â  Â  Â  for _ in range(processed_count):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  next(ds_iterator)
Â  Â  Â  Â  Â  Â  except StopIteration:
Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"All samples for '{split_name}' already processed in previous runs.")
Â  Â  Â  Â  Â  Â  Â  Â  processed_count = total_samples
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â Â 
Â  Â  Â  Â  start_time = time.time()
Â  Â  Â  Â Â 
Â  Â  Â  Â  while processed_count < total_samples:
Â  Â  Â  Â  Â  Â  batch = []
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  for _ in range(batch_size):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  batch.append(next(ds_iterator))
Â  Â  Â  Â  Â  Â  except StopIteration:
Â  Â  Â  Â  Â  Â  Â  Â  pass
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not batch:
Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  processed_batch_list = []
Â  Â  Â  Â  Â  Â  for sample in batch:
Â  Â  Â  Â  Â  Â  Â  Â  if not is_normalized(sample):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sample = normalize_audio_sample(sample)
Â  Â  Â  Â  Â  Â  Â  Â  if not is_correct_sampling_rate(sample):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sample = resample_audio_sample(sample)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # We now keep the numpy array directly
Â  Â  Â  Â  Â  Â  Â  Â  processed_batch_list.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'audio': sample['audio'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'transcription': sample['transcription']
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Create the dataset with the audio column containing the arrays
Â  Â  Â  Â  Â  Â  processed_batch_ds = Dataset.from_list(processed_batch_list)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  with tempfile.NamedTemporaryFile(suffix=".parquet", delete=False) as tmp_file:
Â  Â  Â  Â  Â  Â  Â  Â  processed_batch_ds.to_parquet(tmp_file.name)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  batch_number = processed_count // batch_size
Â  Â  Â  Â  Â  Â  Â  Â  file_name = f"{split_name}/batch-{batch_number:05d}.parquet"
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  max_retries = 5
Â  Â  Â  Â  Â  Â  Â  Â  for attempt in range(max_retries):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"Creating commit for '{file_name}'. Attempt {attempt + 1}/{max_retries}.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  api.create_commit(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  repo_id=full_dest_repo_name,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  repo_type="dataset",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  operations=[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  CommitOperationAdd(path_in_repo=file_name, path_or_fileobj=tmp_file.name)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  commit_message=f"Add processed batch to {split_name}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token=HF_TOKEN,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Success: break from the retry loop
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logging.error(f"Commit failed on attempt {attempt + 1}/{max_retries}: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if attempt < max_retries - 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wait_time = 10 * (2 ** attempt)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"Retrying in {wait_time} seconds...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(wait_time)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logging.critical(f"Failed to commit after {max_retries} attempts. Terminating.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  exit()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  os.remove(tmp_file.name)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  processed_count += len(processed_batch_list)
Â  Â  Â  Â  Â  Â  state[split_name] = processed_count
Â  Â  Â  Â  Â  Â  save_processed_state(state)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  logging.info(f"âœ… Batch pushed for '{split_name}'. Progress: {processed_count}/{total_samples} samples.")

Â  Â  Â  Â  end_time = time.time()
Â  Â  Â  Â  total_time = end_time - start_time
Â  Â  Â  Â  logging.info(f"ðŸ“Š Processing for '{split_name}' completed. Total time: {total_time:.2f} seconds")

# Example usage
if __name__ == "__main__":
Â  Â  SRC_DATASET_NAME = "AhunInteligence/w2v-bert-2.0-finetuning-amharic"
Â  Â  DEST_REPO_NAME = "w2v-bert-2.0-finetuning-amharic-cleaned"
Â  Â Â 
Â  Â  api = HfApi()
Â  Â  repo_url = get_full_repo_name(DEST_REPO_NAME)
Â  Â Â 
Â  Â  max_retries = 5
Â  Â  for attempt in range(max_retries):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  api.create_repo(repo_url, repo_type="dataset", private=False, exist_ok=True, token=HF_TOKEN)
Â  Â  Â  Â  Â  Â  logging.info(f"Successfully created or found destination repository '{repo_url}' on attempt {attempt + 1}.")
Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logging.error(f"Attempt {attempt + 1}/{max_retries} to create repository failed: {e}")
Â  Â  Â  Â  Â  Â  if attempt < max_retries - 1:
Â  Â  Â  Â  Â  Â  Â  Â  logging.info(f"Retrying in 5 seconds...")
Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(5)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  logging.critical(f"Failed to create/find repository '{DEST_REPO_NAME}' after {max_retries} attempts. Terminating.")
Â  Â  Â  Â  Â  Â  Â  Â  exit()
Â  Â Â 
Â  Â  process_and_push_dataset(
Â  Â  Â  Â  src_dataset_name=SRC_DATASET_NAME,
Â  Â  Â  Â  full_dest_repo_name=repo_url,
Â  Â  Â  Â  num_workers=os.cpu_count(),
Â  Â  Â  Â  batch_size=8192
Â  Â  )
